Knowing the different steps of an attack is not only advantageous to the adversary. It is also useful to the defender. The method allows the defender to know how a typical attack will proceed (to a certain degree, without knowing all the details). And by knowing this method, the defender can also structure its defenses. From a defensive point of view, an adversary's attack can be stopped by "breaking" the kill chain.

Reconnaissance Once the objectives are identified, an adversary will start activities by perfonning "Reconnaissance". The goal of the reconnaissance phase is to collect information about the target to increase the chances ofa successful attack.

For adversaries, the goal of the reconnaissance phase is to identify flaws or vulnerabilities that can be abused for the attack.

As a defender, you can reduce the Internet footprint of your company: Investigate what you (and your employees) are exposing online and limit it where required. Furthermore, ensure your employees are aware of the sensitive data they are handling and how they should handle it. An

Once reconnaissance is performed, the adversary hopes to have identi tied a number of (potential) flaws he can leverage. Weaponization is the process of combining an exploit with a suitable vector. An exploit could, for example, be the development of an Office document with malicious macros included. A vector is a delivery mechanism. It can be as simple as attaching the malicious document to an email and sending this to a target company employee's mailbox

It's important to note that the more successful the reconnaissance phase was, the more tailored the weaponization phase can be (and thus: the higher the chance for a successful attack). lfthe adversary knows the target employees are using a specific version of Microsoft Office, he could tailor his attack. Another good example would be knowledge of the installed Anti-Virus software.

The weaponization phase occurs at the attacker's side and is part of his attack planning activities. As long as the exploit is not delivered, there is not much the defender can do. On a continuous basis, the defender should, however, ensure he knows how attack techniques are evolving, so the eventual "attack" does not come as a total surprise.

During the delivery phase, the malicious payload that was prepared in the weaponization phase is delivered to the target. With this step, the attack leaves the planning phase to enter the execution phase.

This delivery can be done through various vectors:

• Sending emails to the victims with malicious payloads (or links to download the payload); • Interact via social media ljke Facebook or Twitter and send malicious links to the victims; • Copy the malicious payload to removable media such as USB sticks, and deliver the media to the victims. • In a watering hole attack, the adversaries will first compromise other, unrelated, websites that tend to be visited by the victims.

End-user awareness is a key security control here: if people understand how advanced adversaries operate, they can be the first layer of defense. Next to end-user awareness, there's also a number of technical controls that can be implemented:

• Mail sandboxes will allow us to investigate incoming email and block malicious attachments or URLs; • IPS engines can block known attack signatures at network level; • Web proxies can be used to block access to suspicious/ malicious websites.

As of this point in the kill chain, defenders have a realistic chance of detecting the incoming attack. It is thus of vital importance to ensure logging & monitoring is well-configured throughout the enterprise. In order to detect advanced adversaries, logging & monitoring should focus both for known bads (signature-based, IOCs ... ) and unknown bads (behavior-based, TIPs ... ).

Upon successful delivery of the payload, exploitation is performed: After the delivery, the malware will be executed on devices used by the victims. When a user received a payload and opens/ runs it, it will run with the privileges of that user. Adversaries often use a combination of technical and people exploits. Consider the example of malicious macro in an office document: it will not run as long as the human doesn't open the document (and enables macros, ifrequired).

As defenders, we have two opportunities during this step to combat adversaries: prevent exploitation and detect exploitation. As part of vulnerability management, we should:

• Harden our systems according to a baseline configuration (shut down unneeded services, change default credentials ... ). • Ensure patches forthird-party software are timely installed; • Ensure the software we develop ourselves is developed according to secure development standards; • Ensure your entire TT environment (including infrastructure and applications) is regularly assessed for vulnerabilities

Application whitelisting can be of great help to prevent arbitrary code execution, but it requires a standardized software environment. If users are expected to install and run their own software according to their business needs, then application whitelisting will be extremely hard to successfully implement.

If exploitation was successful, the adversary will have an initial payload running on the target system. His goal will now be to persist his access to the system (e.g. so the target remains compromised after a reboot).

Windows has a large and diverse set of"autorun" options that can be used for persistence. This can be done in the context of a user, so that persistence is achieved only when the same user logs in again, or in the context of a machine, so that persistence is achieved when the machine is started. Persistence can be as simple as a Start entry in the user's Windows menu configured to nm the malicious payload again or as complex as a dedicated backdoor running as a service or even installed in the firmware of the computer. Webshells are typical backdoors left behind on compromised web servers.

It should be noted that achieving persistence does not necessarily require the malware to be stored in files on the file system. So-called "fileless" malware can achieve persistence by storing commands inside autorun entries in the registry. When executed at startup or login, these commands will inject malicious code inside an existing process.

Often the adversaries will want the malware to have capabilities to be controlled remotely so that they can instruct the malware with the appropriate actions to take. Adversaries attempt to keep this control by the use of a Command & Control inftastructure and channel.

In order to avoid detection and to increase the chances of the outbound connectivity being allowed, adversaries will select a commonly used protocol such as HITP(S), DNS, e-mail or even social media.

Communication between the malware on target systems and adversaries is a good opportunity for us to stop the attack, provided we detect and block it almost instantaneously. We can limit network communications from our network to the Internet via control points like proxies and firewall.

Proxies not only allow us to block and filter traffic, but it also gives us the opportunity to log and inspect the traffic for patterns or anomalies (e.g. beaconing behavior). Beaconing behavior is when malware periodically attempts to connect back to its Command & Control server. If this is done using a fixed time interval, it could form a pattern we can attempt to detect.

Once the adversary has managed to persist malware on an initial system, he can now gear up to start working on his actual objectives.

Once they have a foothold in the infrastructure, adversaries can start a new digital kill chain: they start with reconnaissance of the internal network to identify interesting targets to attack. This will typically be followed by lateral movement. Once inside, lateral movement is often facilitated by the "openness" of the internal network (so-called "egg-shell" problem). Old school design of a secure network puts many of the security controls at the perimeter of the network, and not inside the network. Once adversaries penetrated the perimeter without detection, they encounter fewer obstacles to move inside the internal network.

To be able to detect lateral movement, appropriate controls inside the internal network need to be put in place, such as firewalls and intrusion detection system between different segments of the internal network.

Exfiltration of data is typically a network activity, and as such, leaves traces. Large amounts of data exfiltration (gigabytes or even terabytes) are detectable by graphing the consumed network bandwidth versus a time axis.

The actions taken during the "preparation" or "planning" (e.g. definition of objectives, weaponization) of the attack offer limited opportunities to disrupt the attack. Once the payload / exploit is delivered, the defensive options increase and a broad range of controls can be implemented.

All entry and exit points of your environment should be control points where traffic is monitored. These points are your first line of defense when it comes to keeping adversaries out.

lntemet. The "gates" separating these different network segments should be configured as control points similar to your environment's entry and exit points. They can be seen as the entry/exit points for your different network segments, where certain segments might have stricter security requirements than others.

In addition to network segmentation, systems and data can be grouped into zones as well, for example indicating similar requirements for data confidentiality and integrity. These zones could implement the need for certain commw1ication patterns as well, allowing systems in zoned X to communicate with zone Y, but not with zone Z.

Model 1: Traditional Network Architecture with DMZ Systems in the Inner network just need to access a couple of TCP ports on the servers in the DMZ, thus TCP connections to other ports can all be dropped by the firewalls between the DMZ and the Inner network. This prevents attacks from the Internet or DMZ that use protocols that require other ports, like SMB.

Model 2: Network Architecture with Internal Segregation A solution that is often implemented, is to further divide the lnner network in separate networks and put firewalls in place between the different Inner networks.

Examples of segregation:

Segregate the type of systems: servers in one zone, workstations in another Segregate the type of systems according to business unit: IT, HR, accounting, Segregate the systems according to security level: systems with confidential data, with secret data, with unclassified data,

Model 2: Example Segmentation: Air-Gapped Network Some enterprises will create a network zone that has no physical connections to the internet, or to other zones that have Internet connections. Tt is a network architecture that can be found in intelligence agencies and military organizations and is often referred to as an airgapped network.

Unless you can clearly identify and manage highly confidential data, it is useless to implement a separate network.

Model 3: Network Architecture with Zero Trust- Core Concepts

The "Zero Trust" model is built on the following core concepts: • All resources are accessed in a secure manner, regardless of their location; • Access control is granular and on a "need-to-know" basis and is strictly enforced; • All traffic is inspected & logged. While traditional approaches focus on perimeter traffic inspection & logging, this also includes internal flows.

Key implementation steps include: Identify your sensitive data ("crown jewels") • Understand how this sensitive data flows through your environment • Based on this flow analysis, architect your network accordingly Implement access control & inspection policies • Continuously monitor the environment for unexpected activity

An incredibly big part of defending against advanced adversaries is knowing your own environment. Furthermore, it's important to know what is considered to be "normal" for your organization. Although certain "generic" baselines exist, every organization is different. What is normal for your organization could be highly suspicious to another organization. It is thus important to spend some time "getting to know" your environment.

To defeat advanced adversaries, it's not enough to rely on "known bads". We have to be able to detect the "unknown bads" as well. Advanced adversaries do not only rely on known attack techniques, they are constantly looking for new ways to compromise networks and systems. They want to stay under the radar and avoid detection, that's why they use 0-day exploits and new persistence or exfiltration techniques. lfwe want to be able to successfully detect unknown bad, we should know what the known good is, so we need to know what kind of configuration, status, or behavior is expected and normal.

• On the host-level, you could have a baseline configuration for servers & workstations, where all executables (with their hash) are registered; • On the network-level, you could run a full packet capture to baseline what type of network activity is normal;

All the infom1ation available on the Lntemet about your company or organization defines the "information" footprint. Simply put, your footprint is a measure of your exposure. Another term you might be more familiar with is the "attack surface". The bigger your footprint is, the bigger your exposure and the bigger your attack surface.

For information you control, you can focus on a number of solutions, including: • Increasing security awareness of your staff: They should be made aware and trained about information classification guidelines and publishing policies. Knowledge about scammers should be shared with your staff. • Data classification: It's difficult for staff to understand what they can share if they don't know how data is classified. Ensure all data in the organization is classified and clear rules exist on what these classifications mean. Top Secret information is most likely not intended to be shared on social media. • You could consider implementing DLP solutions, that attempt to stop classified information from leaving the Internet; • Monitor the Internet to see what type of information is available on your corporate web page, social media accounts, partner websites

An inventory of all Internet-facing resources will help you to understand your company's technical footprint. Your company probably already has a list of all its assets used in production, and how they are connected to the Internet. But this list must also include the services running on those assets: server applications, open ports, protocols supported.

Disabling unnecessary services is not just closing ports, but also configure services to limit the features they offer to features that are required for the operation of your company's Internet presence.

All software contains bugs, and many bugs lead to vulnerabilities that can be exploited.

Besides patching, you need to keep up with major releases because old software that is end-of-life is no longer maintained. Besides disabling features and services, you can also protect fi-om the Internet by filtering the network traffic directed to them with firewalls, web application firewalls, intrusion prevention services ... Furthermore, if you cannot block a service from being accessed from the Internet (for valid business reasons), consider implementing increased logging & monitoring on these systems.

Another method to scope your Jntemet footprint is to look up what infonnation about your Internet-facing devices other actors have collected. There are numerous indexing services on the Jntemet that constantly spider the Internet to index information.

Once the reconnaissance activities have been completed, the adversary will attempt to deliver a weaponizcd payload to the target. Typical intrusion methods in use today include: • Malicious e-mail attachments or web pages (watering holes) through (spear) phishing. Due to its success rate and fairly low complexity, this is by far the most common delivery method today. • Abusing a flaw in the external internet perimeter (application or infrastructure level). Due to increased security controls & awareness, this is becoming less frequent. Inserting infected removable media. This would, however, require a form of physical interaction with the target: Compromise third parties in the supply chain and abuse trust relationships. Adversaries could first compromise less secured third parties and use them as a stepping stone towards the actual target

Malicious e-mail attachments and e-mails with malicious URLs are often used as ingress point since every enterprise has to accept e-mail to conduct business. A less obvious ingress point might be USB sticks. In targeted attacks, USB sticks with malicious content have been left in places where staff gathers, such as parking lots.

Since users are still often the weakest link in a company's defenses, it is important to invest in security awareness. Users should be able to recognize a phishing e-mail (or at least a badly structured one) and critically assess the circumstances during which certain suspicious e-mails are received.

Below are some best practices to improve the security posture of your e-mail infrastructure: Limit e-mail relay settings to prevent open relaying. An open e-mail re lay is an SMTP server that allows anyone on the Internet to send e-mail through it and not just e-mail destined to or originating from known users. Implement Sender Policy Framework (SPF) to prevent source e-mail address spoofing. Ensure POP3 & ]MAP authentication and encryption are enforced to avoid unauthorized access or interception of e-mails. Make sure incoming (and to a lesser extent outgoing) e-mails are analyzed by an AV engine or sandbox to avoid malicious attachments reaching your users. Do not accept executable objects as attachment (such as script or binaries). Usually, these file types are not required for business operations.

Malware analysis is often classified in static analysis and dynamic analysis. With dynamic analysis, the sample is executed in a controlled environment and its behavior is observed. Static analysis does not execute the sample but uses techniques like disassembling and decompiling to look at the code of the malware.

The focus of Cuckoo Sandbox is dynamic analysis: the malware sample is executed in a virtual machine; its behavior is observed and a report is produced with a score indicating how confident the Cuckoo Sandbox is about the maliciousness of a sample.

The operating system inside the sandbox is instrumented to increase the capacity of Cuckoo to observe the behavior of a malicious sample. This is done by tracing operating system API calls, capturing and analyzing network traffic, monitoring files, registry, process, and even perform memory dump analysis via Volatility.

To analyze samples that require applications, like PDF files, guests can have new software installed, like Adobe Reader. It is important to select the right version of the supporting application. After execution (or when a time limit is reached), the analysis is terminated, the guest is recycled and the report is produced.

The Cuckoo analysis report is generated upon completion of the analysis and contains the following information (this is a non-exhaustive list). • An overview of matched signatures • An overview of network traffic (this includes a PCAP file, and can be integrated with Suricata) • Files that were created and modified on the guest file system • Process that were created and terminated • A memory dump • Registry entries created and modified

It's possible to create custom signatures to detect malicious behavior that is pertinent to your corporate environment.

Sandboxing technologies have certain shortcomings that we have to understand if we want to defeat advanced adversaries. First of all, certain payloads include various sandbox detection techniques that wi II stop further execution once a sandbox is detected. This allows the attackers to make sure the payload is only executed on "real environments" and attempts to thwart analysis using a sandbox. Targeted payloads take it a step further and might only execute in case certain conditions are met. Instead of simply verifying whether the environment is not sandboxed, it might also check if they are in the correct real environment, such as the correct domain for example. If the computer's domain name does not match the specified domain, execution will be halted. Samples that make use of advanced evasion techniques wi II require additional manual analysis efforts, such as reverse engineering. However, malware sandboxes are still a good way to identify and block the majority of payload deliveries.

YARA uses rules and can scan files or the memory of processes, for matching patterns. YARA rules are written in text, according to a specific syntax and grammar. Essentially, one defines a couple of strings in a rule, and if a file (or memory content) contains these strings, the rule will trigger.

Several anti-virus vendors have adopted the habit of including YA RA rules in the malwarc analysis reports they publish. There are several YARA repositories on GitHub, and the YaraRules Project is an organization dedicated to the sharing of YARA rules (http://yararules.com/).

in YARA it is possible to write a rule that looks for 5 different strings and triggers (i.e. the condition is true) when at least 3 strings are found inside the scanned file.

We need information to build our own YARA rules. For common malware, we can rely on Lndicators Of Compromise published in reports and analysis of malware (sometimes these reports will contain YARA rules that can just be copied).

But for the attacks that we face from persistent adversaries, we will often not be able to rely on open source (or closed source) intelligence for lOCs. We will have to find our own IOCs to create our own YARA rules. Reverse engineering of samples will yield enough lOCs, but reverse engineering can be a daunting task.

YARA can also be used to scan a complete drive. This is achieved by providing the root path to the drive (c:\ in our example), and by using option - r to recurse into the subdirectories.

YARA can also scan the memory of processes (running programs). This is done by passing the process ID (PID) as argument to YARA.YARA must have the privileges to open the process (see example above), for example by running as administrator and elevated. Scanning the memory of a process can help detect strings that are obfuscated or encrypted while at rest in the executable file (PE file), but are in clear-text in memory.

Scanning for strings inside a file, the YARA engine will look for bytes, and not characters. The characters in the string we search for, have to be converted to their binary equivalent (one or more bytes) before they can be searched for. By default, YARA will search for ASCIT strings ifwe provide it with a rule with a string. ASCH striJ1gs take up exactly one byte per character. For example, character A is encoded with byte 0x41 in ASCII.

In YARA, a UNICODE string is defined in a rule by using modifier "wide". For the YARA engine, a UNICODE string is composed of 2 bytes per character: character A is encoded as 0x00 Ox 41 . By default, string searches are case-sensitive: searching for string "Test" will not match byte sequence "test" in a file, only byte sequence "Test". To define a case-insensitive search, we use modifier "nocase" .

To cover as many possible encoding cases as possible, we can use more than one modifier when defining our string $mutex. In this example, we instruct YARA to search for both ASCII and UNICODE encodings of our string by appending keywords "ascii" and "wide" to the definition of string $mutex. We do not have to modify the definition of the string itself. We also make the search case-insensitive, by using keyword "nocase".

A Windows executable (PE file) is a file that follows a well-defined binary format (the PE file format). A PE file starts with characters M and Z (bytes Ox4D and OxSA).

By using condition "$MZ at O", we instruct YARA to look for string MZ at the beginning of the file. I fence our final condition becomes: "$MZ at O and $mutex". This fine-tuned rule no longer triggers on the rule file itself, only on executables.

By creating more generic rules, we have more detections. When we analyze these files that trigger our generic rules, we will notice that we have undesired detections. For example, detections in file types that we are not interested in. This can be fine-tuned by making our rule a bit more specific. This tuning process is necessary to create good YARA rules, but it requires time and resources. This tuning process can be supplemented by using tools to generate YA RA rules automatically based on samples.

Developing YARA rules is a skill, it requires analysis of the samplc(s) to come up with an initial rule, and then several cycles of fine tuning to arrive at a right level of detection. To reduce the cost of this process, and speed up the development of YARA rules, YARA rule generators can be used. A YARA rule generator is a program that takes one or more samples as input, performs an analysis of the samples, and generates a YARA rule to detect the samples and similar samples. lt is important that the generator creates a rule that is not too specific or generic. A

VirusTotal provides an inte resting YARA-based feature to its commercial customer base: (Retro) hunting! • With the hunting function, users can define a number of YARA rules that arc checked for EVERY sample that is uploaded to VirusTotal. • With the retro hunting function, users can search a large data set of old samples (which usually goes back +- 30 days) of uploaded data with a defined set of YARA rules.

Once payloads or samples are identified inside the organization, we could develop YARA rules and use these both in our own environment, but also in the wild (on VirusTotal) to detected related malware samples. These related malware samples could provide additional insights in evolving adversary tactics, which allows us to further improve our defenses!}

Web traffic making use of the I ITIP(S) protocols makes up the largest part of a company's network traffic. As a result, throughout the entire company network, the web protocols are oflen allowed with little to no restrictions. For adversaries, this creates an ideal way of delivering payloads or transporting commands and data to and from a command and control server.

Other ways HTTP(S) can be abused include drive-by downloads, such as those perfonned by exploit kits. These exploit kits attempt to abuse vulnerabilities in user's browsers or browser plugins to download malicious files to the user's computer.

Exploit kits abuse flaws in browsers or browser plugins (Flash, Silverlight, JA YA ... ) in order to drop payloads on victim systems. Some of the more famous exploit kits include Angler, Rig, Terror, and Sundown. Out of these four, Rig is still the most popular and well-known exploit kit and is often used in malvertising and compromised websites campaigns. Typical payloads delivered by exploit kits include ransomware, generic malware, such as a botnets or spyware, and banking trojans.

An exploit kit infection will typically occur in the following way: 1. The victim visits a normal website that is not malicious in itself. 2. A malicious advertisement (malvertisement) hosted on the normal website loads content from a compromised website. 3. The compromised website contains code that performs fingerprinting of the user's browser and plug in to detennine whether it is susceptible to an attack. 4. In case the user is vulnerable, the user is redirected to the exploit kit server, where the exploit code is delivered and the payload is downloaded to the victim's machine and executed.

Some of the security features implemented by proxies include: • Role-based access controls (RBAC) URL categorization • Black/whitelisting • SSL interception • AV engine • File type filtering

Based on a user's role the web proxy can limit access to certain websites or categories of websites. In general, website categories can be used for blacklisting. Through SSL interception, it's possible for the proxy to analyze web communication over HTTPS, which could be useful for scann ing webmail attachments. HTTPS is encrypted and its content cannot be filtered unless the web proxy has TLS interception capabilities. With such interception, the TLS connection is established between the client and the proxy, and the proxy establishes another TLS connection to the web server. The traffic is thus decrypted by the proxy, and available for inspection.

Another, more advanced way of securing web traffic is through the use of an LDS or I PS. These devices will detect and/or prevent payload delivery in the web traffic.

Combining Suricata with the free Emerging Threats rulesets provides a good way of identifying exploit kit activity inside the organization. The ET ruleset contains several built-in rules for detecting exploit kit activity for the most common exploit kits. The mies are frequently updated, allowing you to stay on top of the latest evolution in the exploit kit landscape.

Implementing web filters is "easy" in an enterprise if web proxies are used. The proxy server can operate as a central location where all security decisions are made and where all the filtering can be done. ff no proxy is present, then a less efficient fonn of filtering can be implemented in DNS and firewalls.

Here are a few key best practices to consider when deploying a web proxy: • Enforce all outbound HTTP traffic through the proxy so categorization and blacklisting can be applied on all connections. • Only allow HTI"P-based traffic on the most widely used ports, such as 80, 443, and 8080. • Implement proxy-level authentication to restrict web access to authenticated users and potentially tilter based on user groups. • Make sure detailed proxy logs are generated and retained. These can be useful in case of investigations. • Implement URL categorization and blacklisting to filter unwanted categories.

Execution of PE files and scripts stored on removable storage can be prevented through various means.

Local Group Policies and Active Directory Group Policies can be used to lock down removable storage. Depending on the type of removable storage, it is possible to prevent all access, or only execution, writing or reading. It is also possible to block execution via application whitelisting.

Different vulnerabilities can also lead to code execution upon insertion of removable storage. Known vulnerabilities in Windows have been patched, and a good patching policy will prevent exploitation. But unknown vulnerabilities (zero-day exploits) pose a problem. Also, vulnerabilities in the USB devices and USB protocol can be leveraged to obtain initial intrusion.

Stuxnet used a zero-day vulnerability in the .Ink file format to achieve code execution. A malformed .Ink file with a link to a .dll file, both stored on a USB stick, lead to code execution in the Windows Explorer process when the content of the USB stick was browsed. lt is likely that similar, undiscovered or undisclosed vulnerabilities exist. To try to prevent these types of attacks, more advanced methods are needed, like application whitelisting, or completely blocking untrusted USB devices.

Another type ofUSB danger is a Rubber Ducky. A Rubber Ducky makes use ofa keystroke injection attack, made possible by the trust an OS has for a Human Interface Device (1 IID). In short, a USB device claiming to be a keyboard HID will automatically be detected and accepted by most operating systems. As a result, this type of attack can be executed cross-platform.

The attack can be stopped through application whitelisting and proper software restrictions, for example by blocking cmd.exe in case that is used by the Ducky script. Another cool way to prevent the attack is "Duckhunt" by Pedro M. Sosa. Duckhunt is a daemon script that monitors keyboard usage (speed and selected window for now) and drops or blocks keyboard strokes in case a violation is detected.

VBScript is executed by the Windows Script Host or by the embedded engine ofan application like Internet Explorer. In this course, we will focus on WSH execution. This is the execution of scripts files with an extension like .vbs. The Windows Script Host is implemented in 2 executables: cscript.exe and wscript.exc. Windows executables (PE files) that use the Windows subsystem come in 2 flavors: console executables and GUI executables. Console executables require a console: when a process is created, a console with standard streams STD IN, STDOUT and STDERR is created. GUI executables do not require a console: standard streams are not created, and the executable must render its own GUI if necessary. The console version of the Windows Script Host is implemented in cscript.exe, and the GUI version of the Windows Script Host is implemented in wscript.exe. When a VBScript is executed (for example by double-clicking a .vbs file in Windows Explorer), wscript.exe is launched with the .vbs files as argument. In Microsoft's typicaJ modular design, the VBScript engine is actually not implemented in the Windows Script Host executables wscript.exe and cscript.exe, but in the DLL vbscript.dll.

By focusing on the inner workings of VBScript, we are better able to understand how to detect execution of VBScript and how to prevent it. By default, when a . vbs file is launched, the wscript.exe Windows Script Host executable is executed. By monitoring all processes running on a Windows machine, we can log execution of wscript.exe and cscri-pt.exe and thus record evidence of .vbs file execution. Remark that .vbs is not the only file extension through which VBScripts can be executed. There are many other extensions, for example, .vbe and .wsf. Windows Script Host can be disabled for a particular user by making the following registry changes:

Execution of. vbs files (and other script file extensions) can also be prevented by controlling the execution of wscript.exe and cscript.exe. A rather crude way to achieve this is to remove these executables. A better way is to use ACLs or application whitelisting to control the execution of these host applications. Remember that on 64 bit Windows machines, you have to control both execution of32-bit and 64-bit versions of cscript.exe and wscript.exe.

Often malicious VBS tiles are delivered via e-mail as an attachment. A typical use of VBS in initial intrusion is a down loader. A down loader is a VBScript designed to download an executable from the Internet (often with HTTP), write it to disk and execute it.

Typical malicious scripts will be obfuscated: the code is made less readable to avoid detection by security tools like anti- virus, and to frustrate analyst trying to understand what the VBScript does. In Visual Basic on Windows, 2 common

obfuscation techniques are used: code obfuscation and string obfuscation.

Because Windows executables (PE files) are more and more blocked at the perimeter of enterprise networks (for example by the mail server), adversaries have to find other vectors to deliver their payloads. A vector that has become popular again with malware authors is the Microsoft Office document. Microsoft Office documents (Word text, Excel spreadsheet, PowerPoint slides ... ) not only contain data like text, layout, images ... but can also contain scripts. Such scripts are written in Visual Basic for Applications. VBA is Microsoft's technology to add scripting capabilities to applications. This is not the Windows Script Host, but a completely different ent,,>ine (and another dialect of the Visual Basic language than VBScript) embedded in the application.

VBA programs are not restricted in their access to the operating system's resources. VBA programs can read and write files, change registry values, launch other programs ... all using the same access rights as the user that launched the Word application. Like VBScript, VBA programs can create ActiveX objects (cfr. the downloader example). VBA, however, is more powerful than VBScript, because it can access the Windows API directly and (in theory) have the same capabilities as Windows executables (PE tiles).

Protected View was introduced with MS Office 2010. To contain exploit code from interacting with the resources of the operating system, potentially dangerous documents are opened in a sandbox. This sandbox is restricted from interacting with the resource of the operating system.

VBA code in MS Office documents is often referred to as macros. As automatic execution ofVBA code has inherent risks, Microsoft included MS Office with settings to govern VBA code execution. These settings can be found in the Trust Center.

Together with a new file fonnat (Office Open XML), Microsoft introduced new extensions that allow users and systems to immediately distinguish documents with VBA code (macros) and without. This convention allows for quick triage of documents. An organization could for example setup a mai I server rule that .docm attachments are rejected, while .docx attachments are allowed. MS Office does not allow spoofing of the new macro-less extensions: changing the extension of a .docm tile to docx wi 11 not allow for the execution of the macros. In this case, Word will pennanently disable the macros found in the .docx file.

If exploit code could be made to run with restricted access rights, then exploit code would not be able to access the operating system's resources with the same access level as the user. Sandboxing achieves this by running all potentially vulnerable, complex code of the application in a process with lower access rights than the user. Documents opened in Protected View are also static: active content does not execute, and the user cannot edit the document. To edit the document, the user has to leave Protected View. Leaving Protected View effectively relaunches the MS Office application without a sandbox.

Jobs are Windows objects that contain Windows processes. By default, a Windows process does not run inside a job. Windows processes have to be started or moved inside a job. Jobs can have limits (quotas), for example, the number of active processes that can run inside a job. As exploit code often starts a new process to launch a downloaded executable, this job with a quota will prevent the exploit code from launching its second stage.

Applications like Word will only open potentially dangerous documents in Protected View. Deciding if a document is potentially dangerous or not, is done based on criteria that can be configured by an administrator or a user.

By default, documents that come from outside your organization (downloaded from the Internet or Outlook attachments) or are located in untrusted folders (like the folders for temporary Internet files) are considered potentially dangerous and will be opened in Protected View.

How can an application like Word detennine that a file was downloaded from the Internet, if it was downloaded via another application, for example Internet Explorer? internet-facing applications like Internet Explorer will mark downloaded files according to their origin. This mark is called a mark-of-web. In Windows Explorer, this mark-of-web can be viewed by opening the properties dialog box of the file. For a file with a mark-of-web, the properties dialog box will display a text to inform that this file came from another computer and that it might be blocked.

By default, Microsoft Office applications like Word will not enable VBA code automatically when a document with VBA code is opened for the first time. The user is informed about the presence of macros (VBA code), and the choice to execute macros is left to the user. But once the user has decided to execute macros, the document is considered safe and subsequently Word will always enable macros for this document when it is opened.

The VBA programming language is very powerful because it can also interface directly with the Windows APL This is done through so-called Declare statements. It is unusual for business documents to contain macros that interact with the Windows API. So, the presence of Declare statements is a good indicator for potentially malicious documents. And luckily for us, the Declare keyword cannot be obfuscated.

The 2 major types of malicious scripts used for initial intrusion are downloaders and droppers. Both variants write a payload to disk prior to execution of the payload. This tactic implies that antivirus products have a chance to detect the payload, or that application whitelisting technology can prevent execution of the payload.

To avoid detection when writing to disk, they will rely on more sophisticated types of malicious scripts. Shellcode is one solution employed by adversaries to bypass detection. VBA code can embed shellcode (just like a dropper does), but instead of writing it to disk, shell code is executed immediately in memory. This prevents anti-virus from detecting the malicious shellcode.

Like VBScript, JScript is executed by the Windows Script Host. Host applications wscript.exe and cscript.exe can execute JScript, and the JScript engine is implemented in the jscript.dll.

Malware authors often attack their victims by e-mailing a .js file (a downloader) as an attachment (or inside an attached ZIP file).

Like VBScript, JScript is a powerful language. It can interact with the resources of the operating system like files and registry entries, by creating ActiveX objects. Unlike VBA, JScript cannot interact with the Windows API directly by declaring functions.

JScript supports some powerful functions like the eval function. The eval function takes one string as argument and evaluates the string a JScript code. This allows for the complete obfuscation of a program structure, including the structure of function definitions. By comparison, this is not possible in VBScript, as VBScript lacks the equivalent of the eval function.

If the .js file has a mark-of-web, then the JScript will not execute immediately. The user is presented first with a warning dialog box, infonning the user of the potential dangers of files originating rrom the Internet, and giving the user a choice to open the file or not. Cancel is the default operation.

Mark-of-web also explains why malicious scripts are delivered inside a ZlP file: some ZIP applications do not propagate the mark of web. This means that the attached ZIP file, saved to disk, has a mark-of-web, but when the contained files are extracted, they are not tagged with a mark-of-web.

PowerShell is built on .NET technology, and thus supports the .NET framework for scripting. But it can also use older Microsoft technology like ActiveX and the Windows APL This means that very powerful scripts can be written by adversaries to attack enterprise machines. Scripts can be developed that operate completely in memory, avoiding detection and monitoring.

PowerShell scripts are contained in text files with extension .ps 1. To avoid abuse like with the Windows Script Host, where . vbs, .js ... scripts can be executed just by double-clicking, the .ps I extension is associated with notepad. When a file with extension .ps I is opened in Windows Explorer (for example by double-clicking the icon that represents the file), notepad.exe is launched to edit the file. This prevents attacks similar to e-mailing malicious .vbs or .js files.

The execution of .ps I files is also governed by an execution policy. By default, the execution policy is set to Restricted, and .ps I files cannot be executed when they are loaded into the PowerShell shell.

lt is possible to bypass the PowerShell execution policy by using option - ExecutionPolicy when starting the PowerShell shell (powershell.exe). If this option is given the value Bypass, the execution policy will be bypassed and the script inside the .ps I ti le will execute. A scenario where attackers e-mail users a .ps I file and then instruct the user to save the file to disk, open a command-line to launch PowerShell with the execution policy bypass argument to execute the saved .ps I file is very unlikely. Using .psi files as the initial delivery vector is virtually non-existent, as PowerShell is configured by default to prevent the execution of .ps I files. Only badly configured environments are prone to this attack. However, PowerShell is often used in blended attacks, for example with a malicious Office document (VBA code) or malicious JScript files. In these blended attacks, VBA, VBS or JS is just used to start PowerShell and execute malicious scripts.

Execution of the PowerShell shell can be prevented by blocking powershell.exe. This can be done by various means, like with the prevention of executing any executable: • Removing the powershell.exe • Placing ACLs on powershell.exe • Block powershell.exe with application whitelisting

For many of these solutions, the 32-bit and 64-bit powershell.exe executables need to be taken into account.

Hardentools is a collection of simple utilities designed to disable a number of "features" exposed by operating systems (Microsoft Windows, for now), and primary consumer applications. The intent of this tool is to simply reduce the attack surface by disabling the low-hanging fruit.

What does the tool do? (From https://github.com/securitywithoutborders/hardentools) • Disable Windows Script Host. Windows Script Host allows the execution ofVBScript and JavaScript files on Windows operating systems. This is very commonly used by regular malware (such as ransomware) as well as targeted malware. • Disable AutoRun and AutoPlay. Disables AutoRun / AutoPlay for all devices. For example, this should prevent applications from automatically executing when you plug a USB stick into your computer. • Disable powershell.exe, powershcll_ise.exe and cmd.exe execution via Windows Explorer. You will not be able to use the terminal and it should prevent the use of PowerShell by malicious code trying to infect the system. • Sets User Account Control (UAC) to always ask for permission (even on configuration changes only) and to use "secure desktop". • Disable Macros. Macros are disabled and the "Enable this Content" notification is disabled too, to prevent users from being tricked. • Disable OLE object execution. Microsoft Office applications are able to embed so-called "OLE objects" and execute them, at times also automatically (for example through PowerPoint animations). • Disabling ActiveX. Disables ActiveX Controls for all Office applications. • Disable JavaScript in PDF documents. Acrobat Reader allows executing JavaScript code from within PDF documents. Tb.is is widely abused for exploitation and malicious activity. • Disable execution of objects embedded in PDF documents. Acrobat Reader also allows executing embedded objects by opening them.

Blending scripting technologies has become common practice now: malicious MS Office documents that use YBA to launch PowerShell scripts, PDF documents that contain an embedded Word document with macros.

As defenders, we have two opportunities during this step to combat adversaries: prevent exploitation and detect exploitation.

Application whitelisting can be of great help to prevent arbitrary code execution, but it requires a standardized software environment. If users are expected to install and run their own software according to their business

needs, then application whitelisting will be extremely hard to successfully implement.

Network Access Control (NAC) With NAC, only authorized devices can use the corporate network. The administrator of the network needs to authorize devices, and he / she can deauthorize devices (for example in case of laptop theft). Without authorization, devices are not allowed to connect to the network.

with NAC, it is also possible to allow or deny access to the network for a client based on the properties of the client. With NAC, it is possible to inspect the client, and only allow access if the client has a working antivirus with up-to-date signatures and if the latest patches were applied.

Often, clients who are not compliant with these policies are given access to a remediation network instead of being blocked from network access. In this remediation network, resources are available to update the client to make it compliant with the policies.

OS hardening consists of configuring the operating system and installed programs to reduce the attack surface.

We reduce the attack surface by disabling options and removing features (change default passwords, uninstall / disable unused software ... )

The problem with active features is that they can be attacked. These features create a larger attack surface of our systems, and disabling or removing features reduces the attack surface.

OS and application hardening not only covers options and features to reduce the attack surface but also covers configuration of services to remove unauthenticated access and default credentials.

OS hardening is performed on different types of devices: workstations and servers, but also smartphones and tablets, network devices.VOiP devices ...

Different operating systems need to be considered for hardening: Windows, Linux, OSX, iOS,Android ...

Various checklists and tools are available detailing what configuration changes to make to harden an operating system or application.

The National Institute of Standards and Technology (NIST) publishes checklists that can be used to harden operating systems and applications: • These security checklists are published in the National Checklist Program Repository • The checklists are available in various formats, varying from text for humans to a formalized format for programs

Another source for guidelines to harden Microsoft Windows is Microsoft itself. Microsoft publishes recommended security baselines and security guides. These baselines can be automatically checked and applied with tools offered by Microsoft. One of these tools is the Security Compliance Manager.

New templates can be created too, for custom configurations.

Modem application whitelisting technology takes another approach: rules are used to decide if a program is allowed to run or not. A rule uses criteria to identify to which programs it is applied. Criteria can be: • File name or file path • Digital signature certificates • Hashes

In a whitelisting approach, the convention is to block all programs except the ones explicitly allowed by rules. A black.listing approach is the opposite: All programs are allowed except programs explicitly identified by rules. In general, a whitelisting approach is preferred over a blacklisting approach, as black.listing is more brittle: A single program that is not blacklisted can be used to compromise the system.

Although Software Restriction Policies are the older application whitelisting technology offered by Microsoft., and AppLocker should be the preferred technology to use, SRP has still some advantages, Software Restriction Policies can be applied on versions of Windows that do not support AppLocker. So, in some cases, SRP can be your only option without having to resort to third-party solutions.

Windows only run started programs as long as the user is logged on. If the user logs off ( or the machine is restarted), running programs are terminated. To restart the malicious programs automatically when a user logs on again, persistence mechanisms must be used.

Persistence can be as simple as a Start entry in the user's Windows menu configured to run the malicious payload again, or as complex as a dedicated backdoor running as a service or even installed in the finnware of the computer. Webshells are typical backdoors left behind on compromised web servers.

To achieve persistence on a target system, adversaries must make changes to the configuration of said systems. Not only can many of these changes be prevented by hardening, but they can also be detected by monitoring applications like Microsoft's Sysmon.

Achieving persistence does not necessarily require the malware to be stored in files on the file system. So, called, "fileless" malware can achieve persistence by storing commands inside autorun entries in the registry. When executed at startup or login, these commands will inject malicious code inside an existing process. The malicious code is often stored in an alphanumeric representation in the registry, like BASE64.

Upon successful exploitation, adversaries typically want to persist their access on the target environment (e.g. to survive reboots, user logoff, ... ). Depending on the privileges available to the adversary, they could choose to hide in two main parts of the victim system: • User space: User space is the memory area where application software (and a limited number of drivers) execute. All "user" interactions typically occur in user space. • Kernel: Kernel space is strictly reserved for running a privileged operating system kernel, kernel extensions, and most device drivers. Code used for persistence in kernel-mode is typically referred to as "rootkits", as they interact with low-level parts of the OS and can thus hide themselves better fi-om investigations.

There are however a number of other questions that will determine what type of persistence strategy is opted for by the adversary: • What is the function of the target environment? lfit's a workstation, we can assume that reboots & user logons will occur frequently and thus a persistence strategy related to the user profile could be a viable option. • Is it available from the Internet? If it is, the adversary could opt to place a backdoor in an existing service ( e.g. web shell) and use that to directly reconnect to the system. • Does the adversary have administrative privileges to the system? If administrative privileges are available, the adversary could opt to attempt installing a rootk.it that operates in kernel-mode.

A number of common persistence strategies are listed below. These will be explained in more detail in the following slides. • Web shells: Only useful for Internet-accessible systems. Often used as an initial entry point in an environment. • Task schedules: All major operating systems have task schedulers available (Task Scheduler, At, Cron, ... ). A highly popular means of persistence. • Registry manipulation: A number of registry keys are used during the start-up/ user logon process. These can be abused by adversaries to add malicious code to be executed upon start-up / logon. • Auto-start services: Many systems use a number of services that will automatically launch at start-up (often even with elevated privileges). Adversaries could add additional services that appear to have normal names or functions (will require administrative privileges); • DLL search order hijacking: Highly interesting technique abusing the way Windows prioritizes the loading of DLL's; • User startup folders: much like registry manipulation, adversaries could implement shortcuts/ scripts in a user's startup folder, which is executed upon logon; • Bootkits are used to affect the system upon startup. Malicious code that infects the Master Boot Record (MBR) will run even before the Operating System is launched.

For an adversary to be able to abuse a web shell on a web server, the web shell first has lo be uploaded. lf the file is accessible, but not interpreted as web script, and thus shown back as simple text, the adversary will not be able to execute commands. The web server has to interpret the web shell's script and serve that back to the adversary.

It is easier to prevent web shells from being abused as a persistence mechanism in your server or network than to detect them. If your web application has a file upload function, it should have some restrictions. First of all, it's possible to limit the types of files that can be uploaded (based file header for example). It could be wise to disallow users to upload web scripts of various types.

ln case adversaries do succeed in uploading and abusing a web shell, it is still possible to limit the possible damage they can do. Audit account and group pennissions of the web server's account and make sure it does not have local root privileges or access to unnecessary files and folders.

In case all prevention measures failed, the web shell might still be detected. However, they can be difficult to detect, since they do not initiate connections. The portion of the shell that resides on the web server might also be small and innocent looking. Process monitoring could be used to detect suspicious actions such as command execution or file access outside the web directory.

Utilities such as "at" and "schtasks", along with the Windows Task Scheduler, can be used to schedule programs or scripts to be executed at a date and time or at startup. The account used to create the task must be in the Administrators group on the local system.

Adding an entry to the "Run keys" in the Registry or startup folder will cause the program referenced to be executed when a user logs in. The program will be executed under the context of the user and will have the account's associated pennissions level.

Adversaries may install a new service that can be configured to execute at startup by using utilities to interact with services or by directly modifying the Registry. The service can be disguised by using the name of another, legitimate program.

Instead of creating a new service, an adversary can also modify an existing service to execute and persist the malicious payload. The usage of existing services is a type of masquerading that may make detection analysis more chaJlenging. Modifying existing services could interrupt their functionality or enable services that are disabled or otherwise not commonly used.

Adversaries can perform DLL preloading, by placing a malicious DLL with the same name as a legitimate DLL in a location that Windows searches first. That way, when Windows encounters the malicious DLL, it will be loaded ins1ead of the legitimate one. Adversaries can also replace an existing DLL or modify a manifest file to cause another DLL to load. The malicious DLL can also be configured to load the legitimate DLLs they are meant to replace, which means the application keeps functioning as it normally would.

In order to prevent & detect DLL search order hijacking attacks there are a few controls we can consider: • Certain auditing tools are capable of detecting DLL search order hijacking opportunities. You can use these tools to identify potential DLL search order issues and correct them. An example tool is Powerup (http://www.verisgroup.com/2014/06/17 /powerup-usage/). • Additionally, a whitelisting tool such as AppLocker, capable of blocking unknown DLLs, can be used as well. • Finally, in order to reduce the opportunity of DLL search order hijacking vulnerabilities, ensure the "SafeDLLSearchMode" registry key is enabled. • Disallow loading of remote DLLs, which could take place if an application sets its current directory to a folder on a share. 1n order to detect DLL search order hijacking attacks, the following controls can be considered: • Monitor file systems for moving, renaming, replacing, or modifying DLLs. Changes in the set of DLLs that are loaded by a process (compared with past behavior) that do not correlate with known software, patches, etc., are suspicious. • Monitor DLLs loaded into a process and detect DLLs that have the same file name but abnormal paths. • Modifications to or creation of .manifest and .local redirection files that do not correlate with software updates are suspicious.

A boot sector is a region of a hard drive that contains machine code to be loaded into random-access memory (RAM) by a computer's built-in firmware. The purpose of a boot sector is to allow the boot process of a computer to load a program (usually, but not necessarily, an operating system) stored on the same storage device. A hard drive's boot sectors include the Master Boot Record (MBR) and/or Volume Boot Record (VBR).

The MBR is the section of disk that is first loaded after completing hardware initialization by the BIOS. An adversary who has raw access to the boot drive may overwrite this area, diverting execution during startup from the normal boot loader to adversary code. The MBR passes control of the boot process to the VBR.

Ensure proper permissions are in place to help prevent adversary access to privileged accounts necessary to perform modifications to the boot sectors. Use Trusted Platform Module (TPM) technology and a secure or trusted boot process to prevent system integrity from being compromised.

Perfonn integrity checking on MHR and VBR. To do this, take snapshots of MHR and VBR and compare against known good samples (i.e., a baseline). ln case changes to the MBR or VBR have taken place, further analysis should be performed to determine malicious activity.

Although adversaries are continuously changing & adapting the way they penetrate our networks, they only have a number of options available for persistence. Two main strategies exist to detect persistence strategies in your environment: Host-based agents (IDS, AV, EDR, ... ) can help you detect & alert upon changes to typical persistence locations such as Windows services, startup scripts, scheduled tasks.

Monitor process execution from the Windows Task Scheduler, taskeng.exe, and changes to the Windows Task Scheduler stores (%systemroot%\System32\Tasks) for change entries related to scheduled tasks that do not correlate with known software, patch cycles, etc.

Monitor processes and command-line arguments for actions that could be taken to create tasks. Remote access tools with built-in features may interact directly with the Windows APl to perfonn these functions outside of typical system utilities. Tasks may also be created through Windows system management tools such as Windows Management Instrumentation (WMI) and PowerShell.

Local privilege escalation issues could be caused by outdated I unpatched software that exposes the core OS to vulnerabilities. More often, however, they are the result of a number ofmisconfigurations or mistakes that adversaries can abuse. The below is a non-exhaustive list of some commonly used attack strategies: DLL search order hijacking (which we discussed already in the persistence section of this material); Unquoted paths with spaces Writable Windows Service executables "AlwayslnstallElevated" registry key Unattended install files Group Policy Preferences

If the service path contains spaces and is not surrounded by quotation marks, then Windows has to guess where to find the service executable ... Spaces on the CMD could either be part of the file path OR they could indicate command-line arguments!

An excellent command line to find these types of services on our own machine is below ( originally posted by Danial Compton): wmic service get name,displayname,pathname,startmode lfindstr Ii "Auto" jfindvtr Ii Iv "C:\Windows\\" lfindstr Ii Iv'""'

Unattended installs are often used in enterprise organizations where it would be too time-consuming to perform wide-scale deployments manually. If Windows administrators fail to properly clean up after this process, an XML file called "Unattend.xml" is left on the local system. An example of such a file is included to the left! As you can see, it includes the password in a base64 encoded fonnat, which means it can be very easily decoded. Now, where can you find these xml files? It depends ... Some good candidates to check include: C:\ Windows\Panther\ C:\ Windows\Panther\Unattend\ C:\ Windows\System32 C:\ Windows\System32\sysprep\

ln order to better protect ourselves against these typical privilege escalation attacks, it's a good idea to use tooling that allows us to easily assess how vulnerable we are, so we can fix any identified vulnerabilities. Two tools are very easy to run and use: • BeRoot is a post-exploitation tool to check common Windows misconfiguralions. It can be downloaded as a stand-alone binary. Upon running, it will attempt any privilege escalation techniques and will return a result. • PowerUp is a pure-Powe rShell script that will use tJ1e techniques mentioned above (and much more) to try to escalate privileges.

Given the many different settings available to fine-tune UAC settings in a Windows environment, vulnerabilities due to misconfigurations are bound to arise! Several tools have implemented UAC bypass techniques: In Windows 7, sysprep.exe (used to configure a Windows installation upon install) is vulnerable to a DLL search order hijacking vulnerability, which is exploited by the Metasploit framework "bypassuac" module (for Windows 7) PowerShell Empire uses several bypass UAC techniques. In hardened Windows IO environments (without vulnerabilities), it will spawn a UAC window asking the user to "Permit" a legitimately looking system component from obtaining admin credentials (success rate depends on the UAC settings) • An interesting tool that tries 30+ UAC bypass techniques can be found at https://github.com/hfirefOx/UACME

ln order to avoid detection and to increase the chances of the outbound connectivity being allowed, adversaries will select a commonly used protocol such as HTIP(S), DNS, email or even social media. Cases have also been identified where custom TCP protocols were developed. The endpoint of communication channels are called Command & Control servers; these are servers under the control of the adversaries, but not necessarily owned by the adversaries. ln targeted attacks, adversaries might first compromise other systems and use these as Command & Control servers.

We can limit network communications from our network to the Internet via control points like proxies and firewall.

Proxies not only allow us to block and filter traffic, but it also gives us the oppornmity to log and inspect the traffic for patterns or anomalies (e.g. beaconing behavior). Beaconing behavior is when malware periodically attempts to connect back to its Command & Control server. If this is done using a fixed time interval, it could form a pattern we can attempt to detect.

Where NetFlow (and similar technologies) will capture metadata, full packet capture will capture and store the complete content of network packets. This means that for a TCP connection, we not only have all the metadata like NetFlow would provide, but also the content of the TCP transmission itself: all the data that was transmitted and received by the computer.

When full packet capture is implemented, it is typically done by tapping the network connections at key points in the corporate network infrastructure. These key points are for example the chokepoints where traffic flows between network segments. A good starting point for full packet capture is at the perimeter: capture all traffic between the corporate network and the fnternet.

Because encrypted HTTP traffic becomes the norm on the lntemet, network devices that perform deep inspection of network traffic are becoming "blind". A solution to this problem is TLS interception. A proxy that supports TLS interception works as follows: • When the client starts a TLS connection, the TLS intercepting proxy does not forward the TLS connection to the web server, but it establishes a TLS connection with the client (TLS 1 ). • The client accepts this TLS connection from the proxy because the proxy uses a certificate (with matching private key) that is trusted by the client. This can be done by using a certificate generated by a corporate PK.I that has its root certificate installed on all corporate machines. • Next, the proxy will establish its own TLS connection with the web server (TLS 2). • One set of keys is used to encrypt TLS connection 1, and another set is used to encrypt TLS connection 2. • Since the proxy is now an endpoint for both TLS connections, it can decrypt the traffic from both sides. • The proxy decrypts the traffic from one channel and sends it encrypted in the other channel. This allows the proxy to inspect the "encrypted" traffic, and it can also create a network tap for use by other network devices.

Without corporate proxy and firewalls, the compromised machine would have unrestricted access to the internet, and our adversaries would be able to use any network protocol as command & control channel. With corporate proxy and firewalls, however, internet access is restricted and our adversaries have lo use network protocols that are allowed. Typically, all UDP connections are blocked (blocking DNS) and TCP is only allowed over certain ports, like FTP, HTTP, and HTTPS. If the proxy/firewalls inspect the type of network protocol that is used through the allowed ports, then our adversaries are further restricted in the protocols they can use. For example, SSH over port 80 would be blocked.

To remain undetected when malware uses command & control channels, malware authors have started to adopt a strategy to "blend into the noise". These types of channels can be found in social media applications. There are many social media applications that are very popular with your corporate users, like Facebook, lnstagram, Pinterest, Google+, Dropbox, ... All these social media applications can be used as command and control channel.

The following are some key strategies to block Command & Control communications: • Only allow outbound connectivity for a highly limited number of protocols • Generally speaking, do not allow endpoints themselves to connect outbound, always put in place central control points (e.g. internal DNS servers that will forward outbound DNS, web proxies for HTTP, ... ) • Implement network inspection/ IPS s systems that perform deep packet inspection and detect protocol anomalies (e.g. the use of protocol tunnels for C&C) • Configure network devices to check that the type of traffic matches the port. For example, we only allow HTTP on port 80, we block SSH over port 80 but allow it over port 22.

If we want to start d-::tecting Command & Control activity, we will need to start performing network monitoring & collecting logs from our perimeter devices. Additionally, it might be worth reviewing end-points for applications that are connecting to external systems (this can, for example, be easily achieved using OSQuery, which we will use tomorrow) Furthermore, look for unknown protocols that are not expected in your environment. This is fairly easy, as exotic protocols will raise alerts and are easy to spot. Implement protocol specific gateways (such as outbound web proxies) that can analyze the protocol and analyze protocol fields & settings. Highly popular protocols used for C&C connectivity include HTTP & DNS. In HTTP traffic, interesting fields to analyze include, for example, the User-Agent and the "Referer" request headers. For DNS, we can look at the query length, query types & entropy.

Properly configuring proxies and firewalls to only allow known and trusted protocols and websites, we can further reduce the use of command & control channels.

By restricting the initiation of network connections to devices inside our network, the command & control server cannot connect directly to the compromised machine. It is the compromised machine that has to initiate the connection on a regular basis to check if the command & control server has instructions. This is called beaconing, and by analyzing the frequency of connections, regular beaconing can be detected.

One of the problems we face is that more and more our adversaries will use encrypted command & control channels (like SSL/TLS), which drastically reduces our opportunities to inspect traffic. TLS interception can help us here.

When adversaries reach their targets through lateral movement, they will "finalize the kill". If the objective is espionage, they will collect and exfiltrate data. If the objective is to interfere with the target, they will start making modi f-ications. This can be corrupting, deleting or overwriting of data and systems, or covertly modify data and configurations to change operations within the target.

Data exfiltration: when the objective is to obtain information, it has to be transferred to the adversaries' systems once it is located and accessed. Ex-filtration of data is typically a network activity and as such leaves traces. Large amounts of data exfiltration (gigabytes or terabytes) are detectable by graphing the consumed network bandwidth versus a time axis. Dedicated system can be put in place to monitor for data exfiltration: Data Loss Prevention systems. OLP can be as simple as looking for tags on the network, such as the string "strictly confidential" in uploaded documents. But such simple detections are also simple to bypass. For example, just compressing or encrypting a document before uploading hides all strings inside the document.

How can we prevent lateral movement? • Network segmentation • Harden Active Directory • Limit use & scope of administrative credentials (Identity & Access Management) • Avoid (administrative) credential re-use How can we detect lateral movement? • Attempted network connections to tirewalled network zones • Internal IDS/ JPS technology • Failed access attempts to network shares • Failed login attempts • Implement & monitor deceptive technology (honeypots, canary to_kens, fake tokens, ... )

Windows machines (servers and workstations) were grouped in "domains". Domains are logical groups of machines. They are managed by specialized Windows machines: the domain controller (primary domain controller and backup domain controllers). Domains allowed for centralized management of users and machines.

With the introduction of Active Directory, a hierarchy of domains could be built: a domain tree where trust relationships could be established between domains.

An important feature of Active Directory is Group Policies: these are various configuration settings (for computers and users) that can be configured and managed centrally and applied throughout the members of the domain. A key feature of Active Directory is security: identification, authentication, and authorization of users (and computers). It's this feature of Active Directory that is often attacked by advanced adversaries.

Kerberos is a network authentication protocol based on tickets, developed by MIT. The protocol allows 2 parties (a client and a server for example) to authenticate to each other over an insecure network channel, provided that both parties trust the third party: the Kerberos server.

Each party in a Kerberos environment authenticates to the Kerberos server and receives a ticket. More precisely, this is a ticket-granting-ticket: a ticket that can be used to request tickets. lfKerberos cannot be used (various reasons apply), Windows will fall back to NTLM authentication.

A ticket-granting-ticket is the ticket a client receives first, and it is a special ticket: it is a ticket for the krbtgt service, and can thus be used to request other tickets. By default, a TGT is valid for IO hours. When it expires, Windows will automatically and transparently request a new one.

Once an AD environment is set up, there's a number of items to take into account: LocaJ accounts are difficult to manage at enterprise-level and should typically be avoided. Beware of forgotten local accounts! Even if local privileges have to be configured, it's better to implement & manage these centrally. Adversaries often first obtain local administrator access, after they will use this to escalate to domain (administrative) privileges. Again, it is vital we can limit administrative privileges as much as possible. There is typically no good reason for local end-users to have local administrator privileges. Some organizations choose to set the same complex password for ALL local administrator accounts (for recovery). This is a very bad practice and should be avoided.

There are many attacks possible with Active Directory, we will zoom in on some of the most common attack techniques: • Obtaining access to (a back-up) ntds.dit file • Dumping domain credentials from local systems (memory/ cache) • Passing the hash or ticket • Stealing access tokens using Incognito • Leveraging excessive privileges (e.g. pivoting between users / shares) • NTLMv2 challenge/ response sniffing • Weak passwords / password reuse

Shadow copies (aka VSS, Volume Snapshot Service) is a Microsoft technology to create local backups of files. This technology can be used to recover backup copies of the ntds.dit and SYSTEM files. adversaries don't always need access to a domain controller to obtain these files. When centralized backups are made of the domain controllers, these files will be found on the backup servers too. It is important to adequately protect these files, even in backups.

Once an adversary has obtained a copy of the ntds.dit and SYSTEM files, he can proceed to the extraction of hashes and recovery of passwords. There are several open source tools that can read these files and decrypt hashes: examples are ntdsxtract and secretsdump. For large databases (ten thousand and more users), ntdsxtract tends to be slow (can take several days), but secretsdump is much faster (a couple of hours). Both can output the NTLM hashes and LM hashes (when present) in different formats.

Once adversaries obtain administrative access to a local system (e.g. through one of the privilege escalation techniques we discussed before), he / she could now leverage this to steal local credentials:

Windows endpoints by default store the last IO used domain credentials in DCC format. This is used to authenticate known users when a connection to the domain controller cannot be established. These can be extracted by for example Metasploit.

While domain users are authenticated to a system, their credentials (hashes and sometimes clear-text passwords) are available in memory (lsass.exe process). These can be extracted using well-known tools such as Mimikatz.

Pass-the-hash attacks can only be protected against by preventing the use ofNTLM authentication, or by making sure hashes cannot be obtained. It is possible to disable NTLM authentication via the registry so that only Kcrberos authentication can be used, but in our experience, this is not a viable option for corporate networks. Corporate infrastructure has too many " legacy" systems and applications that require NTLM authentication. With Windows IO Enterprise and Windows 2016 Enterprise, Microsoft introduced a technology called Credential Guard: with Credential Guard properly implemented, typical hash extraction from the memory will be stopped. Note that this requires that all systems in a corporate environment use Windows 10/2016 enterprise without a single exception, otherwise hashes can be stolen from those systems that do not use Windows I 0/2016 enterprise.

Mimikatz is a tool that allows user to execute pass-the-hash attacks by injecting hashes into Windows memory. In the example above, Mimjkatz is executed as administrator and the debug privilege is enabled (privilege::debug command). This is necessary because Mimikatz will write into the LSA process to inject the hash. This cannot be done without administrative rights. The Mimikatz command to start a pass-the-hash attack is sekurlsa::pth. It has 3 mandatory arguments: the usemame (root in our example), the domain name (scc599.private in our example) and the NTLM hash (E19CCF75EE54E06B06A5907AFI3CEF42 in our example, which is the NTLM hash of P@ssw0rd).

In an attack called pass-the-ticket attack. adversaries obtain existing tickets (preferably ticket-granting-tickets from administrative accounts) by extracting them from the memory of compromised machines, and then use them to gain access to other machines. There are open source attack tools available to extract tickets and execute pass-the-ticket attacks .

Typically, an adversary will attempt one of the following tricks to consolidate/ persist administrative access to the AD: Simply creating a domain administrator account (preferably with strong credentials); • Creating a Kerberos golden ticket that will allow long-tenn access to the environment; • Using the DCSync attack; Creating a Skeleton key, which will allow the adversary to authenticate as any user in the environment.

A simple attack to achieve persistence in Active Directory is to create a new domain admin user with a password that never expires.

once a user has the right to create a new user and assign it to arbitrary groups, he can create a domain administrator. This created account will give the adversary domain admin access to the domain as long that the account is not discovered and removed. It is therefore important to monitor your Active Directory infrastructure for the creation of new accounts with administrative rights.

Another method to obtain a ticket for pass-the-ticket attacks is to use Mimikatz to generate a Golden Ticket. A Golden Ticket is a ticket-granting-ticket providing maximum access for a maximum period of time.

The only secret infom1ation that is needed to create a Golden Ticket, is the NTLM hash of the krbtgt account. When Active Directory is compromised, the NTLM hash of the krbtgt account can be extracted from memory.

the NTLM hash of the krbtgt account can also be extracted from the Active Directory database, and its backups. So, it is primordial to protect this data. The only recourse one can have when a Golden Ticket has been generated is to change the password of the krbtgt account. Microsoft has scripts for this.

Each domain controller has a copy of the Active Directory database, and updates to this database on a domain controller (for example the creation of a new user) needs to be propagated to the other domain controllers in due time. This is called Active Directory replication: a set of methods and protocols to synchronize the database of Active Directory domain controllers. Vincent and Benjamin have worked out these methods and protocols for use by Mimikatz: Mimikatz has a command (dcsync) that will make any computer that runs Mimikatz lo impersonate as a Domain Controller to a target Domain Controller to obtain the credentials stored in this Domain Controller.

once an attacker has obtained domain admin credentials, it can use dcsync to connect to a domain controller and extracts the credentials of the krbtgt account. This data can then be used to create a Golden Ticket, and then it is game over: the only recourse you have is to change the krbtgt account password.

It is possible to detect and prevent a dcsync attack. DRSR network traffic should only occur between domain controllers. If you detect DRSR network traffic between a domain controller and a workstation, you know a dcsync attack took place. If you segment your domain controllers in a dedicated network segment, with advanced firewalls as chokepoint between the other network segments, you can block DRSR traffic outside the domain controller network segment.

By carefully designing the network segments that domain controllers will be placed in, we can prevent some attacks. Placing the domain controllers in the same, Aat network as servers and workstations is a bad idea. Domain controllers should have their own LAN segment. Active Directory is also a hierarchical structure: this tree structure is present at many levels: forests, domains, organization units, ... Carefully planning this structure can create a more robust Active Directory structure when security is involved. • Second, it's important to address the issue of re-used local administrator passwords. Microsoft Local Administrator Password Solution (LAPS) provides an effective way of handling this issue by "generating" random local admjnistrator passwords and storing them as part of the domain structure. • CredentialGuard is an excellent new featu re as of Windows IO that will isolate the LSASS process, in an attempt to protect it from "hash dumping" attacks.

A simple solution is to put domain controllers into their own VLAN (Virtual LAN) segment. This should be the minimum: if possible, a physically separated LAN segment offers more security than a VLAN segment Domain controllers of different domains should not be put into the same LAN segment, especially if they do not have the same level of trust, like DMZ and production. Network devices to monitor and control the network traffic can be put in place between the segments. These should be at least firewalls that can block aJI traffic that is not "nonnal" traffic for domain controllers. Kerberos traffic, LDAP, NTLM, ... is considered normal for example, but FTP would not be normal.

Further hardening at the network level can be implemented by using so-called read-only domain controllers. Read-only domain controllers were introduced with Windows 2008. As their name implies, read-only domain controllers only keep a read-only version of the Active Directory database. When replicating Active Directory data with other domain controllers, they will only receive updates, they will never send out updates to the data. Read-only domain controllers can be configured to cache only the credentials of users and computers that try to authenticate to them: these read-only domain controllers do not contain credentials. They will only accept authentication request from a pre-defined set of accounts and will delegate the authentication to full domain controllers.

Here are a few controls to be considered around Domain Controller hardening: First of aJI, we should restrict and closely inspect network connectivity to and from Domain Controllers. An excellent example of this is Internet access. There are very few reasons why domain controllers should be able to (directly) access the Internet; • Secondly, protect the active directory database file (ntds.dit) closely: this includes both in its original location, but also any replicated copies or backups that are spread in the environment. Monitor the use ofntdsutils in your network! • Any unneeded services on the Domain Controller should be disabled. A good example of this is the Microsoft Remote Desktop Protocol (RDP). As we've seen in previous slides, management of the AD can be performed from "Privileged Access Workstations". Furthermore, there should be close to no "extra" software installed on the Domain Controller; • Without exception, restrictive application whitelisting & script restrictions should be enforced on the domain controller; • As the domain controller is the central component and holds "the keys to the castle", it shOLtld be subject to "increased monitoring" as a whole.

Protected processes (and their memory) cannot be accessed by other processes, regardless of the account, they run with. Protected processes were introduced with Windows Vista for ORM purposes (to make media players), but protected processes were repurposed for security boundaries when Windows 8 was introduced. By running the lsass.exe process as a protected process, tools like Mimikatz cannot access the process to extract credentials. ln the screenshot above, we can see that the lsass.exe process running on this machine

is protected: Process Explorer's security tab indicates that the process is protected (PsProtectedSignerLsa- Light).

Protected Processes are implemented in the Kernel software and can thus be defeated ... Mimikatz has a function to remove the protection from protected processes: this "converts" the process into a normal process. To do this, Mimikatz requires its kernel driver to be installed. Installation of this kernel driver can, however, be detected and responded to.

To configure Windows to run the lsass.exe process as a protected process, a change has to be made. By default, Windows will not run the lsass.exe process as a protected process. A registry value has to be created and set: under the registry key HKEY _LOCJ\L_MACHINE\SYSTEM\Cu.rrentControlSet\Control\Lsa, a double word value (DWORD) has to be created. The name of this DWORD is RunAsPPL, and its value should be set to I. Then Windows needs to be rebooted, after which the LSA process will run as a protected process.

In order to understand Credential Guard, we first need to understand the

normal architecture of Windows. The simplified diagram above describes this architecture. At the lowest level, we have the hardware. On top of that runs a hypervisor: software that uses the hardware to provide virtualization services. And then we have the Windows operating system. Essentially, Windows has two important parts: the kernel and userland. Processes run in userland, drivers run inside the kernel. The LSA process runs in userland.

When Credential Guard is enabled, Windows still runs on top of the hypervisor and the hardware, and the LSA process still runs in userland. The difference, however, is that the credentials are no longer stored inside this LSA process (lsaas.exe). With Credential Guard, the credentials are stored in the Isolated LSA process (Lsalsol.exe). This process does not run under Windows but in the Virtual Secure Mode. This is a separate, virtualized environment, that is separated from the other environments (like Windows) via hardware.

importance of Identity & Access Management: • Use onJy accounts with the least privileges able to do the job: Create fine-grained administrative accounts (e.g. database server administrators, web server administrators, workstations administrators, ... ) AND educate your staff on how administrative privileges should be used! A classic mistake is to have helpdesk users wield Domain Administrator accounts to fix small issues on an employee's workstation. Use only accounts with the least privileges able to do the job. • Let domain admin and enterprise admin accounts-only logon to domain controllers and dedicated workstations (see "Privileged Access Workstations" architecture).

Windows events have a well-defined format and are stored in files called event logs, depending on their "source". For example, events that originate from the Windows kernel when Windows is started are stored in the System event log, while error events from applications (without dedicated event log) are stored in the Application event log. By default, event logs are stored locally in folder C:\Windows\System32\winevt\Logs\.

There are a couple of native Windows tools to view event logs. There is the event viewer, a graphical user interface tool that is a snap-in for the Microsoft Management Console (MMC). Wevutil.exe is a command line tool. Event logs can also be viewed with PowerSheLI and many third-party tools, like Sysintemals' PsLogList.

Event viewer is adequate for ad-hoc browsing, but it is not well adapted to search efficiently through a large number of events. Some fom1 of automatic processing is required for emcicnt bunting.

Before attackers can ex filtrate data, they have to identify and locate it. This may sound obvious, but in a large organiz.ation, it might require some work to sift through files and data to find what attackers are looking for.

cmd.exe offers various commands, like the dir command. Dir stands for directory: it returns the content of a directory by listing all the files and directories inside a directory, together with some metadata such as the file size.

To search through a complete filesystem, the document to search for should be prefixed with the root directory of the file drive to be searched. For example, for drive C:, the command is "dir Is c:\secret.doc". PowerShell can, of course, do similar things as cmd.exe. One command that can be used to locate files, is GetChildltem. Get-Childitem takes many options. The -Path option allows us to specify where to start searching. In this example, we search in the c:\research-and-development directory. We can filter for specific names with the - Include option: -Include •confidential* will select all files (and directories) with the string confidential in the filename.

With option - File we search only for files and ignore directories (e.g. directories that match the name *confidential* will not be listed). By default, the Get-Childitem command only searches in the provided directory, and not the underlying directories. To achieve searching through sub directories, option Recurse must be provided.

Meterpreter is a dedicated command-line interpreter with specialized penetration testing commands. The Meterpreter code is not written to disk, but it is injected in an existing process and thus leaves no footprints on the disk.

In order to prevent adversaries from stealing sensitive data, it's important for an organization to know what data they possess & that it is correctly classified. Based upon this classification, user access rights should be highly limited, and users should only have access to what they need to fulfill their daily jobs ("need-to-know" principle).

It is a fact that searching through a complete file system is "very noisy". When we search for files inside a file system with cmd.exe or PowerShell, these programs will open all directories to list the files and directories inside it. Depending on the number of files and directories inside a file system, this can require opening ten thousand or more directories, and thus produce a considerable number of activities. Searching with an index is different: the file system does not need to be searched through, only the index itself.

A possible solution to the problem of monitoring complete file systems is to limit the number of files we monitor, and devise methods to lead attackers to these files. One method to achieve this is the use of decoy files .

Once interesting information is located, the data is typically centralized to an internal system that has already been compromised. Here, the data can be prepared to be exfiltrated: • It can be compressed or split in smaller chunks to hinder detection; • It can be encrypted to hide the contents of the data that is being exfiltrated Given these features, we often see that typical (portable) archiving utilities are used to prepare the data, as they offer both use cases described above. Good candidates include: • 7z • RAR

As a detection strategy, it might be a good idea to keep an eye out for archival tools being used in the environment. One concrete way of doing so would be to attempt detection of archival tools by filtering the Windows command line logging for typical "archival" syntaxes (archive file extensions, tool command line syntaxes, ... ).

One very popular avenue for data exfiltration is the use of cloud-based file hosting and file sharing services. Many of them have a fi-ee tier. Popular examples are: • OneDrive • Dropbox • Google Drive

It is possible to configure proxies and next-generation firewalls to block access to these services, and we highly recommend that you would do this for your corporate environment. These services are the goto-service for data exfiltration.

If your corporate network is connected to the Internet, it is impossible to completely prevent data exfiltration. Our corporate network relies too much on diverse protocols and services, that it is impossible to properly prevent abuse on all protocols and services.

While prevention of data exfiltration is extremely hard, detection of data exfiltration offers us a bit more hope.

Data Loss Prevention (DLP) solutions are put in place at the perimeter of the corporate network, where they can observer network traffic leaving the corporate network to the Internet. With signature-based detection, DLP solutions will detect exfiltration ofconfidential data via watennarks, similar in the way anti-virus and IDS work to detect malicious activity based on signatures. With behavior-based detection, DLP solutions will detect ex:filtration of confidential data by observing abnormal network traffic patterns. This is in large part based on the volume of the data.

The DLP solution must be able to illspect traffic in different encodings, and also support various compression methods. Because bypassing detection would otherwise be trivial by compressing the data (or using another encoding that obfuscated the markers). When attackers encrypt the data to exfiltrate, they could be able to bypass signature-based DLP solutions.

The volume of the data that leaves the corporate network is an important indicator for behavior based OLP solutions. They will measure the amount of data per user and per destination, and alert on unexpectedly large data transfers. This method, of course, can only be successful if the exfiltrated data is indeed large enough to deviate from the norm. l fit is small, this method will not detect it.

Protocols like DNS provide an interesting option for adversaries to include covert payloads. Should adversaries want to use this type of strategy however, this would typically rely on "strange" DNS traffic: The ratio of A-records vs TXT-records should show that TXT records occur far less in the environment. A high volume ofTXT records could indicate a DNS tunnel; • High entropy in DNS names could reveal randomly generated domain names; • BASE64 is a preferred encoding mechanism used by many adversaries. Data could be exfiltrated by splitting it over different BASE64 strings; • Generally speaking, DNS tunneling would most likely lead to long DNS names being resolved; We could also focus on the volumes of traffic: • A high volume of DNS requests from I source should be a source for investigation; • A high volume of DNS requests to I domain; • A high number of hosts resolved per domain;

The adversary wants to be as quiet as possible in our networks and systems (e.g. not generate events and certainly not alerts), so that we will not detect them and that they have more time to perform their malicious deeds. We want the adversary to make more "noise", e.g. generate more alerts, and we will achieve this by tricking the adversary.

A honeypot is a decoy system that looks enticing to adversaries. They are designed to attract attention so that adversaries will try to interact with them. This interaction can be normal usage or attacks to the operating system or services of the honey pot.

Honeypots have to be discovered by the adversaries when they search through our networks, and by assigning them enticing names, we try to increase the chances that adversaries will try to access them.

Honeypots should not be able to be used by adversaries to pivot to other systems: particular care should be taken that a honeypot does not contain tools and services and has access to other systems so that it can be used as a pivot.

Another popular method to trick adversaries are canaries. A canary is decoy data that looks enticing to adversaries. We place canaries in places where they can be found and accessed by our adversaries when they search through our corporate assets. Interacting with a canary will trigger an alert. The name canary refers to the canaries used in coal mines to alert miners for poisonous gas emanations.

the Active Directory is the central point in most organization environments from which crucial security functions take place. W11ile moving through the network, adversaries will query the Active Directory to find interesting user accounts ... This opens an opportunity for us! We can create a canary by adding a user account in Active Directory that looks enticing to our potential attackers, for example, BackupAdmin or any other account name that might look important in your corporate environment and would be attractive to attackers to compromise.

In summary, tricbng the adversary will help us slow down the attack and improve our chances of detection. Two interesting technologies can help us achieve this goal: honeypots (decoy systems) & canaries (decoy data)! Honeypots are decoy systems: they simulate systems or services and are designed to try to attract the attention of our adversaries. Canaries are decoy data: they simulate corporate data and documents that look enticing to our adversaries and are designed to alert us when this data is consumed.

a threat is established by evaluating the following components: Capabi I ity: The threat actor is capable of reaching his / her objectives Intent: The threat actor is deliberately trying to attain his / her objectives Opportunity: Certain conditions exist that could allow a threat actor to reach his / her objectives

we will define threat intelligence as: "Analyzed information about the hostile intent, capability, and opportunity of an adversary"

• Tactical intelligence: Attacker methodologies, tools & tactics. Often referred to as TTP's, these include typical "habits" of adversaries, without providing narrow Indicators of Compromise

• Operational intelligence: At the lowest level, we have operational threat intelligence, which is often limited to highly technical / narrow Jndicators of Compromise of specific attacks / attack campaigns

Here are a few excellent use cases for threat intelligence: • Immediately use fresh JOCs in real-time prevention & detection tools (Firewalls, IDS, IPS, SIEM, ... ). This is what many organizations already do in an automated fashion. Many firewalls & SIEMs, for example, come with a built-in "intelligence feed", which is typically just a long list oflOCs; • Use old IOCs to cross-check archived logs for potential hits and thus signs of previous compromises (which could still be active today!). In many organizations, this is one of the main reasons for incident discovery; • Use tactical threat intelligence (e.g. TTPs) to create new hypotheses for threat hunting

An IOC scanner is a tool that will use IOCs to scan computer systems' resources (file system, registry, memory,

Loki is a command-line based tool used to scan for IOCs. It is written in Python, and in this screenshot, we see the compiled version in action on Windows.

When Loki is started without arguments or options, it will perfonn a full scan of the machine it runs on.

we can see that Loki is based on YARA ruJes. These YARA rules are part of the signatures downloaded by Loki from the Loki GitHub repository and are initialized at program startup.

To maximize the chances of Loki to perfonn a successful scan, it must be executed with an account that has administrative rights.

Loki is a scanner that just detects resources that match IOCs, like files: it will not delete or clean a malicious file like an anti-virus would do.

Lok.i will only perform process memory scans when it has the debug privilege (e.g. is running under an administrative account). If it is running as a normal user, it will not perfonn memory scans (even limited to processes of the same account as the one executing Loki). Loki will use YARA rules to scan the process memory and it will also report on processes that have open ports, i.e. that are listening for network connections.

we can see that Loki looks for a large amount of the following IOC types: Filenames (with regular expression patterns) Command & Control server indicators MD5 hashes SHA! hashes SHA256 hashes It even has a whitelist: false positive hashes.

Loki can also produce a fonnat better suitable to automatic processing with the CSV option .

While real-time detection is based on known bads, threat hunting is based on anomalies. Real-time detection takes a narrow view of our corporate infrastructure using rules. Threat hunting tries to see the bigger picture and looks for anomalies and strange behavior inside our corporate infrastructure.

Critical success factors for threat hunting are: • Experienced analysts that know how attacks work and what to look out for. These people should also understand your environment and know what your crown jewels are. A good knowledge of offensive security methods is required to be a good threat hunter: a poacher makes the best gamekeeper. Meanwhile, these people should also understand your environment, which is not an easy "blend". • A large collection of logs that are being generated throughout different parts of your environment. This includes any type of logs (Windows event logs, firewalls, ... ) Threat hunting relies heavily on logs generated by different systems in your corporate network. These logs are not always enabled by default, so you might need to revise your logging & monitoring strategy. • A large, centralized, data repository that can be used to collect available logs for your environment. We cannot rely on a myriad of different log types spread across our environment. In order to effectively hunt, we need to collect all logs centrally, so we can easily search, query and analyze them.

Threat hunting should be an iterative process, where the following actions are performed: I. Define hypothesis: Define a hypothesis that can be tested (e.g. "Adversaries are attempting to infiltrate our organization using phishing emails"); 2. Perform analysis: Test the hypothesis by analyzing the available logs; 3. Identify patterns: Through your analysis, identify potential patterns of malicious behavior; 4. Provide feedback: Based on the results of the hunt, provide feedback
